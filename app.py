import os
from dotenv import load_dotenv
from pydantic import BaseModel
from fastapi import FastAPI, HTTPException
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from operator import itemgetter
from faq_tool import get_faq_context
from fastapi.middleware.cors import CORSMiddleware


# Carrega vari√°veis de ambiente
load_dotenv(dotenv_path=".env")

# 1. Configura√ß√£o do Modelo de Linguagem (LLM)
llm_fast = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0,
    google_api_key=os.getenv("GEMINI_API_KEY")
)

# 2. Defini√ß√£o do Prompt do Sistema
system_prompt_faq = ChatPromptTemplate.from_messages([
    ("system",
     """
### PAPEL
Voc√™ deve responder perguntas sobre o documento oficial (trechos fornecidos em CONTEXTO).  
Se a informa√ß√£o n√£o estiver no documento, diga de forma educada:  
"N√£o tem essa informa√ß√£o no nosso FAQ, mas posso te ajudar a procurar se quiser üòä"

### ESTILO DE COMUNICA√á√ÉO
- Seja gentil, acolhedor e natural ‚Äî como algu√©m explicando com calma.
- Pode cumprimentar o usu√°rio brevemente (ex: "Oi!", "Tudo bem?").
- Evite linguagem t√©cnica ou formal demais.
- Seja claro e direto, mas sempre simp√°tico.
- Se o texto mencionar partes do documento, fale apenas do conte√∫do ‚Äî sem citar se√ß√µes, n√∫meros ou t√≠tulos.
- Nunca invente informa√ß√µes ou tire conclus√µes fora do que est√° no contexto.

### ENTRADA
- ROUTE=faq  
- PERGUNTA_ORIGINAL=...  
- PERSONA=... (define o tom e concis√£o)  
- CLARIFY=... (se preenchido, responda isso primeiro)
"""
),
    ("human",
     "Pergunta do usu√°rio:\n{question}\n\nCONTEXTO (trechos do documento):\n{context}\n\nResponda apenas com base no CONTEXTO, seguindo o tom acolhedor e claro descrito acima.")
])

prompt_faq = ChatPromptTemplate.from_messages([
    system_prompt_faq,
    ("human",
     "Pergunta do usu√°rio:\n{question}\n\n"
     "CONTEXTO (trechos do documento):\n{context}\n\n"
     "Responda com base APENAS no CONTEXTO."
     )
])

# 3. Cadeia RAG (Retrieval-Augmented Generation)
faq_chain_core = (
    RunnablePassthrough.assign(
        question=itemgetter("input"),
        context=lambda x: get_faq_context(x["input"])
    )
    | prompt_faq
    | llm_fast
    | StrOutputParser()
)

def answer_question(question, session_id="default"):
    return faq_chain_core.invoke(
        {"input": question},
        config={"configurable": {"session_id": session_id}}
    )

# 4. Inicializa√ß√£o do FastAPI
app = FastAPI(
    title="SaveIt FAQ AI API",
    description="API para responder perguntas sobre o SaveIt usando RAG (Retrieval-Augmented Generation) e Google Gemini.",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 5. Defini√ß√£o do Schema de Requisi√ß√£o
class QuestionRequest(BaseModel):
    question: str
    session_id: str = "WEB" # Opcional, mantido para compatibilidade

# 6. Defini√ß√£o do Endpoint
@app.post("/faq")
async def faq_endpoint(request_data: QuestionRequest):
    if not request_data.question:
        raise HTTPException(status_code=400, detail="Campo 'question' √© obrigat√≥rio.")
    try:
        # A fun√ß√£o answer_question √© s√≠ncrona, mas o FastAPI a gerencia em um thread pool
        resp = answer_question(request_data.question, session_id=request_data.session_id)
        return {"answer": resp}
    except Exception as e:
        # Em caso de erro (ex: chave da API inv√°lida, problema no RAG), retorna 500
        raise HTTPException(status_code=500, detail=f"Erro interno do servidor: {str(e)}")
